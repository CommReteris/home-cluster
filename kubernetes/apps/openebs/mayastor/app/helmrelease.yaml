---
apiVersion: helm.toolkit.fluxcd.io/v2beta1
kind: HelmRelease
metadata:
  name: mayastor
  namespace: openebs
spec:
  chart:
    spec:
      chart: mayastor
      reconcileStrategy: ChartVersion
      sourceRef:
        kind: HelmRepository
        name: mayastor
  install:
    createNamespace: true
  interval: 1m0s
  values:
    image:
      registry: docker.io
      repo: openebs
      tag: v2.0.0
      pullPolicy: IfNotPresent

    nodeSelector:
      kubernetes.io/arch: amd64

    earlyEvictionTolerations:
      - effect: NoExecute
        key: node.kubernetes.io/unreachable
        operator: Exists
        tolerationSeconds: 5
      - effect: NoExecute
        key: node.kubernetes.io/not-ready
        operator: Exists
        tolerationSeconds: 5

    base:
      metrics:
        enabled: true
        pollingInterval: "5m"
      jaeger:
        # -- Enable jaeger tracing
        enabled: false
        initContainer: true
        agent:
          name: jaeger-agent
          port: 6831

    operators:
      pool:
        logLevel: info
        resources:
          limits:
            cpu: "100m"
            memory: "32Mi"
          requests:
            cpu: "50m"
            memory: "16Mi"

    jaeger-operator:
      name: "{{ .Release.Name }}"
      crd:
        install: false
      jaeger:
        create: false
      rbac:
        clusterRole: true

    agents:
      core:
        logLevel: info
        resources:
          limits:
            cpu: "1000m"
            memory: "128Mi"
          requests:
            cpu: "500m"
            memory: "32Mi"
      ha:
        enabled: true
        node:
          logLevel: info
          resources:
            limits:
              cpu: "100m"
              memory: "64Mi"
            requests:
              cpu: "100m"
              memory: "64Mi"
        cluster:
          logLevel: info
          resources:
            limits:
              cpu: "100m"
              memory: "64Mi"
            requests:
              cpu: "100m"
              memory: "16Mi"

    apis:
      rest:
        logLevel: info
        replicaCount: 1
        resources:
          limits:
            cpu: "100m"
            memory: "64Mi"
          requests:
            cpu: "50m"
            memory: "32Mi"
        # Rest service parameters define how the rest service is exposed
        service:
          type: ClusterIP
          nodePorts:
            http: 30011
            https: 30010

    csi:
      image:
        registry: registry.k8s.io
        repo: sig-storage
        pullPolicy: IfNotPresent
        provisionerTag: v2.2.1
        attacherTag: v3.2.1
        registrarTag: v2.1.0
      controller:
        logLevel: info
        resources:
          limits:
            cpu: "32m"
            memory: "128Mi"
          requests:
            cpu: "16m"
            memory: "64Mi"
      node:
        logLevel: info
        topology:
          segments:
            openebs.io/csi-node: mayastor
          nodeSelector: false
        resources:
          limits:
            cpu: "100m"
            memory: "128Mi"
          requests:
            cpu: "100m"
            memory: "64Mi"
        nvme:
          io_timeout: "30"
          ctrl_loss_tmo: "1980"
          keep_alive_tmo: ""
        kubeletDir: /var/lib/kubelet
        pluginMounthPath: /csi
        socketPath: csi.sock

    io_engine:
      logLevel: info,io_engine=info
      api: "v1"
      target:
        nvmf:
          # -- NVMF target interface (ip, mac, name or subnet)
          iface: ""
          # -- Reservations Persist Through Power Loss State
          ptpl: true
      # -- Pass additional arguments to the Environment Abstraction Layer.
      # Example: --set {product}.envcontext=iova-mode=pa
      envcontext: ""
      reactorFreezeDetection:
        enabled: false
      # -- The number of cpu that each io-engine instance will bind to.
      cpuCount: "2"
      # -- Node selectors to designate storage nodes for diskpool creation
      # Note that if multi-arch images support 'kubernetes.io/arch: amd64'
      # should be removed.
      nodeSelector:
        openebs.io/engine: mayastor
        kubernetes.io/arch: amd64
      resources:
        limits:
          cpu: ""
          memory: "1Gi"
          hugepages2Mi: "2Gi"
        requests:
          cpu: ""
          memory: "1Gi"
          hugepages2Mi: "2Gi"

    etcd:
      # Pod labels; okay to remove the openebs logging label if required
      podLabels:
        app: etcd
        openebs.io/logging: "true"
      replicaCount: 3
      # Kubernetes Cluster Domain
      clusterDomain: cluster.local
      # TLS authentication for client-to-server communications
      # ref: https://etcd.io/docs/current/op-guide/security/
      client:
        secureTransport: false
      # TLS authentication for server-to-server communications
      # ref: https://etcd.io/docs/current/op-guide/security/
      peer:
        secureTransport: false
      # Enable persistence using Persistent Volume Claims
      persistence:
        # -- If true, use a Persistent Volume Claim. If false, use emptyDir.
        enabled: true
        # -- Will define which storageClass to use in etcd's StatefulSets
        # a `manual` storageClass will provision a hostpath PV on the same node
        # an empty storageClass will use the default StorageClass on the cluster
        storageClass: ""
        # -- Volume size
        size: 2Gi
        reclaimPolicy: "Delete"
      # -- Use a PreStop hook to remove the etcd members from the etcd cluster on container termination
      # Ignored if lifecycleHooks is set or replicaCount=1
      removeMemberOnContainerTermination: false

      # -- AutoCompaction
      # Since etcd keeps an exact history of its keyspace, this history should be
      # periodically compacted to avoid performance degradation
      # and eventual storage space exhaustion.
      # Auto compaction mode. Valid values: "periodic", "revision".
      # - 'periodic' for duration based retention, defaulting to hours if no time unit is provided (e.g. 5m).
      # - 'revision' for revision number based retention.
      autoCompactionMode: revision
      # -- Auto compaction retention length. 0 means disable auto compaction.
      autoCompactionRetention: 100
      extraEnvVars:
        # -- Raise alarms when backend size exceeds the given quota.
        - name: ETCD_QUOTA_BACKEND_BYTES
          value: "8589934592"

      auth:
        rbac:
          create: false
          enabled: false
          allowNoneAuthentication: true

      # Init containers parameters:
      # volumePermissions: Change the owner and group of the persistent volume mountpoint to runAsUser:fsGroup values from the securityContext section.
      volumePermissions:
        # chown the mounted volume; this is required if a statically provisioned hostpath volume is used
        enabled: true
      # extra debug information on logs
      debug: false
      initialClusterState: "new"
      # Pod anti-affinity preset
      # Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
      podAntiAffinityPreset: "hard"
      service: # etcd service parameters defines how the etcd service is exposed
        type: ClusterIP
        port: 2379
        nodePorts:
          # Port from where etcd endpoints are accessible from outside cluster
          clientPort: 31379
          peerPort: ""

    loki-stack: # -- Enable loki log collection for our components
      enabled: true
      loki:
        rbac:
          create: true
          pspEnabled: false
        enabled: true # -- Enable loki installation as part of loki-stack
        persistence:
          enabled: true
          # -- StorageClass for Loki's centralised log storage
          # empty storageClass implies cluster default storageClass & `manual` creates a static hostpath PV
          storageClassName: ""
          reclaimPolicy: "Delete"
          size: 10Gi
        # loki process run & file permissions, required if sc=manual
        securityContext:
          fsGroup: 1001
          runAsGroup: 1001
          runAsNonRoot: false
          runAsUser: 1001
        # initContainers to chown the static hostpath PV by 1001 user
        initContainers:
          - command: ["/bin/bash", "-ec", "chown -R 1001:1001 /data"]
            image: docker.io/bitnami/bitnami-shell:10
            imagePullPolicy: IfNotPresent
            name: volume-permissions
            securityContext:
              runAsUser: 0
            terminationMessagePath: /dev/termination-log
            terminationMessagePolicy: File
            volumeMounts:
              - mountPath: /data
                name: storage
        config:
          # Compactor is a BoltDB(loki term) Shipper specific service that reduces the index
          # size by deduping the index and merging all the files to a single file per table.
          # Ref: https://grafana.com/docs/loki/latest/operations/storage/retention/
          compactor:
            # Dictates how often compaction and/or retention is applied. If the
            # Compactor falls behind, compaction and/or retention occur as soon as possible.
            compaction_interval: 20m
            # If not enabled compactor will only compact table but they will not get
            # deleted
            retention_enabled: true
            # The delay after which the compactor will delete marked chunks
            retention_delete_delay: 1h
            # Specifies the maximum quantity of goroutine workers instantiated to
            # delete chunks
            retention_delete_worker_count: 50
          # Rentention period of logs is configured within the limits_config section
          limits_config:
            # configuring retention period for logs
            retention_period: 168h
        service:
          type: ClusterIP
          port: 3100
          # Port where REST endpoints of Loki are accessible from outside cluster
          nodePort: 31001

      promtail:
        rbac:
          create: true
          pspEnabled: false
        # -- Enables promtail for scraping logs from nodes
        enabled: true
        # -- Disallow promtail from running on the master node
        tolerations: []
        config:
          # -- The Loki address to post logs to
          lokiAddress: http://{{ .Release.Name }}-loki:3100/loki/api/v1/push
          snippets:
            # Promtail will export logs to loki only based on based on below
            # configuration, below scrape config will export only our services
            # which are labeled with openebs.io/logging=true
            scrapeConfigs: |
              - job_name: {{ .Release.Name }}-pods-name
                pipeline_stages:
                  - docker: {}
                kubernetes_sd_configs:
                - role: pod
                relabel_configs:
                - source_labels:
                  - __meta_kubernetes_pod_node_name
                  target_label: hostname
                  action: replace
                - action: labelmap
                  regex: __meta_kubernetes_pod_label_(.+)
                - action: keep
                  source_labels:
                  - __meta_kubernetes_pod_label_openebs_io_logging
                  regex: true
                  target_label: {{ .Release.Name }}_component
                - action: replace
                  replacement: $1
                  separator: /
                  source_labels:
                  - __meta_kubernetes_namespace
                  target_label: job
                - action: replace
                  source_labels:
                  - __meta_kubernetes_pod_name
                  target_label: pod
                - action: replace
                  source_labels:
                  - __meta_kubernetes_pod_container_name
                  target_label: container
                - replacement: /var/log/pods/*$1/*.log
                  separator: /
                  source_labels:
                  - __meta_kubernetes_pod_uid
                  - __meta_kubernetes_pod_container_name
                  target_label: __path__
    obs:
      callhome:
        enabled: false
